{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ignore warning messages if you see any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 How a Neural Network Makes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input/response mapping \n",
    "def f(x):\n",
    "    return np.where(np.dot(x, np.random.rand(3, 1)) > 0.8, 1., 0.)\n",
    "    \n",
    "X = np.random.random((1000, 3))\n",
    "y_true = f(X)\n",
    "\n",
    "# print(X[:3])\n",
    "# print(y_true[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network topology:\n",
    "\n",
    "<img src=\"images/Network.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "neural_network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer1 = Dense(units=2, activation=\"relu\", input_dim=3)\n",
    "neural_network.add(hidden_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer2 = Dense(units=3, activation='relu')\n",
    "neural_network.add(hidden_layer2)\n",
    "\n",
    "# output_layer = Dense(units=2, activation=\"sigmoid\")\n",
    "# neural_network.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize model architecture\n",
    "neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's manually set the network's weights \n",
    "# (input neuron weights, bias weights)\n",
    "\n",
    "## input --> hidden layer 1\n",
    "random_weights1 = [np.random.random((3, 2)), np.ones(2)]\n",
    "neural_network.layers[0].set_weights(random_weights1)\n",
    "\n",
    "## hidden layer 1 --> hidden layer 2\n",
    "random_weight2 = [np.random.random((2, 3)), np.ones(3)]\n",
    "neural_network.layers[1].set_weights(random_weight2)\n",
    "\n",
    "## hidden layer 2 --> output layer\n",
    "# random_weight3 = [np.random.random((3, 2)), np.ones(2)]\n",
    "# neural_network.layers[2].set_weights(random_weight3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred y:\t[0 0 0 0 0 0 0 0 0 0]\n",
      "True y:\t[1. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# predict y from a few X's\n",
    "X_sample = X[:10,:]\n",
    "y_sample_pred = neural_network.predict_classes(X_sample)\n",
    "y_sample_true = y_true[:10]\n",
    "print(\"Pred y:\\t{}\".format(y_sample_pred.reshape(-1)))\n",
    "print(\"True y:\\t{}\".format(y_sample_true.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive accuracy: 49.4%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on predictive accuracy for all X\n",
    "y_pred = neural_network.predict_classes(X)\n",
    "acc = sum(y_true == y_pred.reshape(-1))[0]/float(y_true.shape[0])*100\n",
    "print(\"Predictive accuracy: {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. How to Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network topology:\n",
    "\n",
    "<img src=\"images/Network.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "smart_neural_network = Sequential()\n",
    "\n",
    "# smart_neural_network.add(Dense(2, activation='relu', input_dim=3))\n",
    "# smart_neural_network.add(Dense(3, activation='relu'))\n",
    "# smart_neural_network.add(Dense(2, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "smart_neural_network.compile(metrics=['accuracy'],\n",
    "                             loss='binary_crossentropy',\n",
    "                             optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing: convert responses to 2d categorical vectors\n",
    "from keras.utils import np_utils\n",
    "y_categorical = np_utils.to_categorical(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/GradientDescent.png\" width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 987us/step - loss: 0.6975 - acc: 0.4890\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 158us/step - loss: 0.6936 - acc: 0.4815\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 155us/step - loss: 0.6916 - acc: 0.5070\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 149us/step - loss: 0.6903 - acc: 0.5265\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 222us/step - loss: 0.6894 - acc: 0.5355\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 142us/step - loss: 0.6886 - acc: 0.5430\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 153us/step - loss: 0.6878 - acc: 0.5520\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 174us/step - loss: 0.6870 - acc: 0.5555\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 150us/step - loss: 0.6860 - acc: 0.5610\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 217us/step - loss: 0.6849 - acc: 0.5705\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 133us/step - loss: 0.6834 - acc: 0.5730\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.6821 - acc: 0.5790\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 130us/step - loss: 0.6807 - acc: 0.5805\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.6791 - acc: 0.5905\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 149us/step - loss: 0.6776 - acc: 0.5945\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 147us/step - loss: 0.6757 - acc: 0.6000\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 154us/step - loss: 0.6737 - acc: 0.6075\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s 154us/step - loss: 0.6716 - acc: 0.6110\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 162us/step - loss: 0.6692 - acc: 0.6240\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 143us/step - loss: 0.6667 - acc: 0.6300\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 0.6640 - acc: 0.6385\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 152us/step - loss: 0.6615 - acc: 0.6435\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.6583 - acc: 0.6520\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s 161us/step - loss: 0.6550 - acc: 0.6615\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.6512 - acc: 0.6660\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.6477 - acc: 0.6750\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.6442 - acc: 0.6815\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s 137us/step - loss: 0.6406 - acc: 0.6895\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.6368 - acc: 0.6920\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.6329 - acc: 0.7025\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 0.6288 - acc: 0.7110\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 136us/step - loss: 0.6249 - acc: 0.7155\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.6200 - acc: 0.7195\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.6146 - acc: 0.7240\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.6100 - acc: 0.7285\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 158us/step - loss: 0.6051 - acc: 0.7310\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.6003 - acc: 0.7330\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 0.5951 - acc: 0.7360\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.5897 - acc: 0.7400\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 212us/step - loss: 0.5842 - acc: 0.7440\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 151us/step - loss: 0.5788 - acc: 0.7475\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 149us/step - loss: 0.5735 - acc: 0.7520\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 130us/step - loss: 0.5677 - acc: 0.7560\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 153us/step - loss: 0.5617 - acc: 0.7605\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 0.5559 - acc: 0.7665\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.5504 - acc: 0.7730\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.5442 - acc: 0.7800\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 0.5381 - acc: 0.7850\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.5317 - acc: 0.7905\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 134us/step - loss: 0.5255 - acc: 0.7935\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 136us/step - loss: 0.5185 - acc: 0.7960\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 133us/step - loss: 0.5116 - acc: 0.7995\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.5047 - acc: 0.8010\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 156us/step - loss: 0.4976 - acc: 0.8025\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 143us/step - loss: 0.4905 - acc: 0.8055\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 0.4835 - acc: 0.8080\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s 137us/step - loss: 0.4761 - acc: 0.8140\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 151us/step - loss: 0.4693 - acc: 0.8175\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s 148us/step - loss: 0.4614 - acc: 0.8190\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s 187us/step - loss: 0.4544 - acc: 0.8205\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.4472 - acc: 0.8280\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s 142us/step - loss: 0.4402 - acc: 0.8310\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 0.4325 - acc: 0.8345\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 147us/step - loss: 0.4246 - acc: 0.8375\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.4169 - acc: 0.8420\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 137us/step - loss: 0.4085 - acc: 0.8510\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 120us/step - loss: 0.4007 - acc: 0.8555\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.3924 - acc: 0.8615\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s 154us/step - loss: 0.3837 - acc: 0.8615 0s - loss: 0.4008 - acc: 0.85\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.3753 - acc: 0.8675\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 0.3668 - acc: 0.8695\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s 159us/step - loss: 0.3585 - acc: 0.8725\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.3500 - acc: 0.8765\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 128us/step - loss: 0.3423 - acc: 0.8805\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 0.3340 - acc: 0.8850\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s 148us/step - loss: 0.3253 - acc: 0.8880\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 0.3165 - acc: 0.8925\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.3081 - acc: 0.8935\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 0.2988 - acc: 0.8980\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s 130us/step - loss: 0.2901 - acc: 0.9040\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s 158us/step - loss: 0.2821 - acc: 0.9050\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s 173us/step - loss: 0.2739 - acc: 0.9080\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 0.2658 - acc: 0.9120\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.2575 - acc: 0.9200\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.2493 - acc: 0.9260\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 0.2414 - acc: 0.9290\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.2337 - acc: 0.9310\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 150us/step - loss: 0.2263 - acc: 0.9325\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 153us/step - loss: 0.2195 - acc: 0.9420\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.2127 - acc: 0.9450\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.2061 - acc: 0.9455\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.1999 - acc: 0.9460\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s 136us/step - loss: 0.1949 - acc: 0.9510\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.1892 - acc: 0.9510\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.1843 - acc: 0.9510\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.1791 - acc: 0.9570\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.1741 - acc: 0.9560\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 134us/step - loss: 0.1697 - acc: 0.9630\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s 131us/step - loss: 0.1651 - acc: 0.9675\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.1613 - acc: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10abcf890>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for a number of rounds\n",
    "intial_weights = smart_neural_network.get_weights()\n",
    "smart_neural_network.fit(X, y_categorical, \n",
    "                         epochs=100,\n",
    "                         batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(smart_neural_network.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 243us/step\n",
      "Predictive accuracy on training data: 0.97\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model accuracy on training data\n",
    "score = smart_neural_network.evaluate(X, y_categorical)\n",
    "print('Predictive accuracy on training data:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But how well can our neural network do on data it's never seen before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "num_training_examples = 800\n",
    "\n",
    "X_train = X[:num_training_examples]\n",
    "y_train = y_categorical[:num_training_examples]\n",
    "X_valid = X[num_training_examples:]\n",
    "y_valid = y_categorical[num_training_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 1s 940us/step - loss: 0.6982 - acc: 0.4713\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6950 - acc: 0.4738\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.6929 - acc: 0.4756\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.6914 - acc: 0.4944\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.6904 - acc: 0.5156\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.6897 - acc: 0.5281\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.6870 - acc: 0.560 - 0s 124us/step - loss: 0.6892 - acc: 0.5344\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6887 - acc: 0.5406\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6881 - acc: 0.5487\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.6876 - acc: 0.5469\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6869 - acc: 0.5550\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.6863 - acc: 0.5575\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6856 - acc: 0.5594\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6847 - acc: 0.5681\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6836 - acc: 0.5656\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.6825 - acc: 0.5737\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6814 - acc: 0.5763\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6802 - acc: 0.5837\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 154us/step - loss: 0.6790 - acc: 0.5825\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.6776 - acc: 0.5919\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6762 - acc: 0.5944\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.6746 - acc: 0.6069\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.6731 - acc: 0.6112\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6715 - acc: 0.6150\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6696 - acc: 0.6181\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6676 - acc: 0.6256\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6657 - acc: 0.6356\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6634 - acc: 0.6388\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.6609 - acc: 0.6450\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6582 - acc: 0.6500\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6554 - acc: 0.6606\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6523 - acc: 0.6675\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6493 - acc: 0.6731\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.6461 - acc: 0.6812\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.6430 - acc: 0.6875\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6398 - acc: 0.6919\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.6369 - acc: 0.7056\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.6337 - acc: 0.7037\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6301 - acc: 0.7094\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.6266 - acc: 0.7119\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6231 - acc: 0.7162\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6198 - acc: 0.7181\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6166 - acc: 0.7175\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.6131 - acc: 0.7219\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6090 - acc: 0.7250\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.6053 - acc: 0.7275\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6014 - acc: 0.7312\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.5973 - acc: 0.7350\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.5931 - acc: 0.7375\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.5888 - acc: 0.7381\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.5844 - acc: 0.7431\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.5801 - acc: 0.7456\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.5757 - acc: 0.7500\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.5717 - acc: 0.7531\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.5672 - acc: 0.7563\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.5624 - acc: 0.7594\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.5573 - acc: 0.7669\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.5523 - acc: 0.7688\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.5475 - acc: 0.7744\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.5425 - acc: 0.7794\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.5373 - acc: 0.7825\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.5321 - acc: 0.7850\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.5270 - acc: 0.7919\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.5219 - acc: 0.7950\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.5166 - acc: 0.7981\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.5114 - acc: 0.7987\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.5059 - acc: 0.8013\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.4999 - acc: 0.8025\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.4943 - acc: 0.8062\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.4882 - acc: 0.8119\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.4820 - acc: 0.8113\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.4759 - acc: 0.8163\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.4693 - acc: 0.8194\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.4632 - acc: 0.8225\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.4572 - acc: 0.8219\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.4510 - acc: 0.8294\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.4450 - acc: 0.8306\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.4387 - acc: 0.8337\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.4327 - acc: 0.8375\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.4265 - acc: 0.8431\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.4203 - acc: 0.8450\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.4136 - acc: 0.8531\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 123us/step - loss: 0.4067 - acc: 0.8588\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.4003 - acc: 0.8612\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.3944 - acc: 0.8650\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.3878 - acc: 0.8681\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.3808 - acc: 0.8694\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 109us/step - loss: 0.3738 - acc: 0.8731\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 105us/step - loss: 0.3670 - acc: 0.8744\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.3609 - acc: 0.8744\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.3543 - acc: 0.8769\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.3470 - acc: 0.8769\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.3396 - acc: 0.8844\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.3334 - acc: 0.8856\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 107us/step - loss: 0.3263 - acc: 0.8894\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.3191 - acc: 0.8950\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.3129 - acc: 0.8962\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.3059 - acc: 0.9006\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.2987 - acc: 0.9044\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.2914 - acc: 0.9088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10abe2090>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-train only on training examples\n",
    "smart_neural_network.set_weights(intial_weights)\n",
    "\n",
    "smart_neural_network.compile(metrics=['accuracy'], loss='binary_crossentropy', optimizer='rmsprop')\n",
    "smart_neural_network.fit(X_train, y_train, batch_size=32, epochs=100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 820us/step\n",
      "Predictive accuracy on validation data: 0.97\n"
     ]
    }
   ],
   "source": [
    "# evaluate on validation (never-seen-before) examples \n",
    "validation_scores = smart_neural_network.evaluate(X_valid, y_valid)\n",
    "print('Predictive accuracy on validation data:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a neural network model with a \"deeper\" architecture (e.g. more than just 1-2 hidden layers). Fit this on the training data from the input/response example given in the notebook and evaluate the predictive error. \n",
    "\n",
    "* Experiment with the number of neurons in each layer.\n",
    "* When training, increase the number of epochs \n",
    "\n",
    "Do you experience any performance gains? Does training run slower than it did before? If so, by how much? Are the performance gains worth it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Sample solution:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_neural_network = Sequential()\n",
    "# deep_neural_network.add(Dense(128, activation='relu', input_dim=3))\n",
    "# deep_neural_network.add(Dense(64, activation='relu'))\n",
    "# deep_neural_network.add(Dense(32, activation='relu'))\n",
    "# deep_neural_network.add(Dense(2, activation='sigmoid'))\n",
    "# deep_neural_network.compile(metrics=['accuracy'], loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# deep_neural_network.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "# score = deep_neural_network.evaluate(X_valid, y_valid)\n",
    "\n",
    "# print 'Predictive accuracy on validation data:', score[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
